{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 4 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Mikhail Pronin lab03').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-5.newprolab.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Mikhail Pronin lab03</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f52d4b6e2e8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2022-01-06 18:46 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2022-01-06 18:46 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2022-01-06 18:46 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2022-01-06 18:46 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "items = spark.read.option(\"delimiter\",\"\\\\t\").csv(\"/labs/slaba03/laba03_items.csv\", header=True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.withColumn(\"item_id\",col(\"item_id\").cast(IntegerType()))\\\n",
    "             .withColumn(\"channel_id\",col(\"channel_id\").cast(IntegerType()))\\\n",
    "             .withColumn(\"datetime_availability_start\",col(\"datetime_availability_start\").cast(DateType()))\\\n",
    "             .withColumn(\"datetime_availability_stop\",col(\"datetime_availability_stop\").cast(DateType()))\\\n",
    "             .withColumn(\"datetime_show_start\",col(\"datetime_show_start\").cast(DateType()))\\\n",
    "             .withColumn(\"datetime_show_stop\",col(\"datetime_show_stop\").cast(DateType()))\\\n",
    "             .withColumn(\"content_type\",col(\"content_type\").cast(IntegerType()))\\\n",
    "             .withColumn(\"year\",col(\"year\").cast(IntegerType()))\\\n",
    "             .withColumn(\"genres\",col(\"genres\").cast(StringType()))\\\n",
    "             .withColumn(\"region_id\",col(\"region_id\").cast(IntegerType()))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1916"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.agg({\"year\": \"min\"}).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.csv(\"/labs/slaba03/laba03_train.csv\", header=True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn(\"user_id\",col(\"user_id\").cast(IntegerType()))\\\n",
    "             .withColumn(\"item_id\",col(\"item_id\").cast(IntegerType()))\\\n",
    "             .withColumn(\"purchase\",col(\"purchase\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.filter(train.purchase == 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = spark.sparkContext.parallelize(train_data).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=1654, item_id=9897, purchase=1),\n",
       " Row(user_id=1654, item_id=7394, purchase=1),\n",
       " Row(user_id=1654, item_id=9064, purchase=1),\n",
       " Row(user_id=1654, item_id=73216, purchase=1),\n",
       " Row(user_id=1654, item_id=88816, purchase=1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_purchase = train_data.groupBy(\"user_id\").sum(\"purchase\").alias(\"user_purchase\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_purchase = spark.sparkContext.parallelize(user_purchase).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "newColumns_user = ['user_id', 'cnt_purchase_user']\n",
    "user_purchase = user_purchase.toDF(*newColumns_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_purchase = train_data.groupBy(\"item_id\").sum(\"purchase\").alias(\"item_purchase\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_purchase = spark.sparkContext.parallelize(item_purchase).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "newColumns_items = ['item_id', 'cnt_purchase_movie']\n",
    "item_purchase = item_purchase.toDF(*newColumns_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train.join(user_purchase, on = 'user_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.join(item_purchase, on = 'item_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.na.fill(value=0, subset=[\"cnt_purchase_movie\"])\n",
    "train = train.na.fill(value=0, subset=[\"cnt_purchase_user\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_f = f.udf(lambda r : Vectors.dense(r),VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog = spark.read.csv(\"/labs/slaba03/laba03_views_programmes.csv\", header=True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog = prog.withColumn(\"user_id\",col(\"user_id\").cast(IntegerType()))\\\n",
    "           .withColumn(\"item_id\",col(\"item_id\").cast(IntegerType()))\\\n",
    "           .withColumn(\"ts_start\",col(\"ts_start\").cast(IntegerType()))\\\n",
    "           .withColumn(\"ts_end\",col(\"ts_end\").cast(IntegerType()))\\\n",
    "           .withColumn(\"item_type\",col(\"item_type\").cast(StringType()))\\\n",
    "           .withColumn(\"watching_time\",col(\"ts_end\") - col(\"ts_start\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "watching_time = prog.groupBy([\"user_id\"]).sum(\"watching_time\").alias(\"watching_time\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "watching_time = spark.sparkContext.parallelize(watching_time).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_watched = prog.groupBy([\"user_id\"]).count().alias(\"movies_watched\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_watched = spark.sparkContext.parallelize(movies_watched).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "watching_time = watching_time.join(movies_watched, on = \"user_id\", how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.join(watching_time, on = \"user_id\", how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.join(items[['item_id', 'year']], on = 'item_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "newColumns = ['user_id','item_id','purchase','cnt_purchase_user','cnt_purchase_movie','watching_time', 'count_watched', 'year']\n",
    "train = train.toDF(*newColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train.na.fill(value=0, subset=[\"year\"])\n",
    "train = train.na.fill(value=0, subset=[\"watching_time\"])\n",
    "train = train.na.fill(value=0, subset=[\"count_watched\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, item_id: int, purchase: int, cnt_purchase_user: bigint, cnt_purchase_movie: bigint, watching_time: bigint, count_watched: bigint, year: int]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn(\"decade\",(f.floor((col(\"year\")-1900)/10)).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'item_id',\n",
       " 'purchase',\n",
       " 'cnt_purchase_user',\n",
       " 'cnt_purchase_movie',\n",
       " 'watching_time',\n",
       " 'count_watched',\n",
       " 'year',\n",
       " 'decade']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import isnan, when, count, col\n",
    "#train.select([count(when(col(c).isNull(), c)).alias(c) for c in train.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    " \n",
    "featuresCols = train.columns\n",
    "featuresCols.remove('purchase')\n",
    "featuresCols.remove('user_id')\n",
    "featuresCols.remove('item_id')\n",
    " \n",
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[vectorAssembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pipelineModel.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, item_id: int, purchase: int, cnt_purchase_user: bigint, cnt_purchase_movie: bigint, watching_time: bigint, count_watched: bigint, year: int, decade: int, features: vector]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train.sampleBy(\"purchase\", fractions={0: 0.8, 1: 0.8}, seed=5757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = train.join(train_dataset, on=[\"user_id\", \"item_id\"], how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(maxIter=8, maxDepth=5, labelCol=\"purchase\", seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"features\", \n",
    "                                          labelCol=gbt.getLabelCol(),\n",
    "                                          metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtModel = gbt.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gbtModel.transform(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC curve: 0.825882829162064\n"
     ]
    }
   ],
   "source": [
    "print(\"Area under ROC curve:\", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.read.csv(\"/labs/slaba03/laba03_test.csv\", header=True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.withColumn(\"user_id\",col(\"user_id\").cast(IntegerType()))\\\n",
    "             .withColumn(\"item_id\",col(\"item_id\").cast(IntegerType()))\\\n",
    "             .withColumn(\"purchase\",col(\"purchase\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=1654, item_id=94814, purchase=None),\n",
       " Row(user_id=1654, item_id=93629, purchase=None),\n",
       " Row(user_id=1654, item_id=9980, purchase=None),\n",
       " Row(user_id=1654, item_id=95099, purchase=None),\n",
       " Row(user_id=1654, item_id=11265, purchase=None)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.join(user_purchase, on = 'user_id', how = 'left')\n",
    "test = test.join(item_purchase, on = 'item_id', how = 'left')\n",
    "test = test.join(watching_time, on = \"user_id\", how = 'left')\n",
    "test = test.join(items[['item_id', 'year']], on = 'item_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.na.fill(value=0, subset=[\"year\"])\n",
    "\n",
    "test = test.withColumn(\"decade\",(f.floor((col(\"year\")-1900)/10)).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Columns = ['item_id','user_id','purchase','cnt_purchase_user','cnt_purchase_movie','watching_time', 'count_watched', 'year','decade']\n",
    "test = test.toDF(*test_Columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.na.fill(value=0, subset=[\"watching_time\"])\n",
    "test = test.na.fill(value=0, subset=[\"count_watched\"])\n",
    "test = test.na.fill(value=0, subset=[\"cnt_purchase_user\"])\n",
    "test = test.na.fill(value=0, subset=[\"cnt_purchase_movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_prediction_data = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_submit = gbtModel.transform(final_prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_submit = predictions_submit[['user_id','item_id','probability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr=udf(lambda v:float(v[1]),FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_submit = predictions_submit.withColumn(\"purchase_try\", tr('probability'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_submit = predictions_submit[['user_id','item_id','purchase_try']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_columns = ['user_id', 'item_id', 'purchase']\n",
    "predictions_submit = predictions_submit.toDF(*final_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=5385, item_id=728960, purchase=0.07331158220767975),\n",
       " Row(user_id=8484, item_id=728960, purchase=0.07331158220767975),\n",
       " Row(user_id=72926, item_id=728960, purchase=0.07331158220767975),\n",
       " Row(user_id=89053, item_id=728960, purchase=0.07349050790071487),\n",
       " Row(user_id=94988, item_id=728960, purchase=0.07574103772640228)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_submit.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, item_id: int, purchase: float]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_submit = predictions_submit.sort(predictions_submit.user_id.asc(),\n",
    "                                             predictions_submit.item_id.asc()).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1654</td>\n",
       "      <td>336</td>\n",
       "      <td>0.073641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1654</td>\n",
       "      <td>678</td>\n",
       "      <td>0.073641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1654</td>\n",
       "      <td>691</td>\n",
       "      <td>0.073641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1654</td>\n",
       "      <td>696</td>\n",
       "      <td>0.073668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1654</td>\n",
       "      <td>763</td>\n",
       "      <td>0.073641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  purchase\n",
       "0     1654      336  0.073641\n",
       "1     1654      678  0.073641\n",
       "2     1654      691  0.073641\n",
       "3     1654      696  0.073668\n",
       "4     1654      763  0.073641"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_submit.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_submit.to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
